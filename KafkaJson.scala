package BasicTasks.sparkStreaming
import org.apache.spark.sql.{SaveMode, SparkSession}import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._import org.apache.spark.SparkConfimport org.apache.spark.streaming.{Seconds, StreamingContext}import org.apache.spark.streaming.kafka.KafkaUtilsobject KafkaJson1 { //   case class aslcc (name:String, age:Int, city:String)    def main(args: Array[String]) {      import scala.util.Try      val topics =  Try(args(0).toString).getOrElse("today12")      val brokers = Try(args(1).toString).getOrElse("localhost:9092")      val topicSet = topics.split(",").toSet      val kakfkaParams = Map[String, String]("metadata.broker.list" -> brokers)      val conf = new SparkConf().setAppName("cassandraSpark").setMaster("local[2]")      val ssc = new StreamingContext(conf, Seconds(20))      val sc = ssc.sparkContext      import kafka.serializer.StringDecoder      // rdd of json dstreams      val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kakfkaParams, topicSet)      lines.foreachRDD { rdd =>        import org.apache.spark.sql.SparkSession        val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()        import spark.implicits._        //key, value        val rdd1 = rdd.map(x=>x._2)        val df1=spark.read.json(rdd1).toDF()        df1.createOrReplaceTempView("tab")        val x = spark.sql("select * from tab")        x.show()        x.write.mode(SaveMode.Append).option("delimiter","|").csv("/tmp/jsonTab.csv")      }      ssc.start      ssc.awaitTermination()  }}